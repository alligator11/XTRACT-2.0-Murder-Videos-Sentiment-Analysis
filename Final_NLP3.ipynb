{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaSngNkBK8b"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from requests.models import MissingSchema\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import urllib3, socket\n",
        "from urllib3.connection import HTTPConnection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AybDrPjoL8iQ"
      },
      "outputs": [],
      "source": [
        "def syllable_and_count(word ,syllable_count,syllable_word_count,complex_word):\n",
        "    count = 0\n",
        "    syllable = []\n",
        "    vowels = \"aeiou\"\n",
        "    for i in word:\n",
        "      if i in vowels:\n",
        "        syllable.append(i)\n",
        "        count+=1\n",
        "\n",
        "    if (word[-2:]==\"ed\") or (word[-2:]=='es'):\n",
        "      count=-1\n",
        "    if count<0:\n",
        "      count = 0\n",
        "    if count>0:\n",
        "      syllable_count+=len(set(syllable))\n",
        "      if len(set(syllable))>=2:\n",
        "        complex_word+=1\n",
        "    syllable_word_count+=1\n",
        "\n",
        "    return (complex_word,syllable_count,syllable_word_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "files_stopwords = [\"Stopwords/StopWords_Auditor.txt\",\n",
        "                   \"Stopwords/StopWords_Currencies.txt\",\n",
        "                   \"Stopwords/StopWords_DatesandNumbers.txt\",\n",
        "                   \"Stopwords/StopWords_Generic.txt\",\n",
        "                   \"Stopwords/StopWords_GenericLong.txt\",\n",
        "                   \"Stopwords/StopWords_Geographic.txt\",\n",
        "                   \"Stopwords/StopWords_Names.txt\"]\n",
        "\n",
        "for file in files_stopwords:\n",
        "    try:\n",
        "        w = open(\"/content/{filename}\".format(filename=file), 'r', encoding='utf-8').read().strip().lower()\n",
        "    except UnicodeDecodeError:\n",
        "        w = open(\"/content/{filename}\".format(filename=file), 'r', encoding='latin-1').read().strip().lower()\n",
        "    en_stopwords.update(w.split())\n",
        "\n",
        "extra_stopwords = [\"./td-block-span12\",\n",
        "                   \"/.content\",\n",
        "                   \"./td-related-span4\",\n",
        "                   \"./block\",\n",
        "                   \"./block1\",\n",
        "                   \"/.td-pb-row\",\n",
        "                   \"/.td-container\",\n",
        "                   \"/.post\"]\n",
        "\n",
        "en_stopwords.update(extra_stopwords)\n",
        "\n",
        "print(len(en_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaIAyaFO-CdX",
        "outputId": "16b7541e-9dea-41c1-dcb3-16683259929a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3gtkHfEDCHs",
        "outputId": "0fa05743-bc28-4e7b-81c4-34181b05d835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'shirey',\n",
              " 'stahl',\n",
              " 'davidson',\n",
              " 'larue',\n",
              " 'cerda',\n",
              " 'antony',\n",
              " 'deloitte',\n",
              " 'sims',\n",
              " 'waits',\n",
              " 'clifford',\n",
              " 'dew',\n",
              " 'garver',\n",
              " 'shae',\n",
              " 'santana',\n",
              " 'timmerman',\n",
              " 'joeann',\n",
              " 'hann',\n",
              " 'huang',\n",
              " 'beatrice',\n",
              " 'cristy',\n",
              " 'thirty',\n",
              " 'lon',\n",
              " 'katina',\n",
              " 'tenorio',\n",
              " 'cambridge',\n",
              " 'maciel',\n",
              " 'stouffer',\n",
              " 'tommie',\n",
              " 'behr',\n",
              " 'novella',\n",
              " 'lemus',\n",
              " 'edyth',\n",
              " 'bannister',\n",
              " 'vanetta',\n",
              " 'goebel',\n",
              " 'groce',\n",
              " 'wild',\n",
              " 'gibson',\n",
              " 'hafer',\n",
              " 'hawthorne',\n",
              " 'camila',\n",
              " 'idell',\n",
              " 'suanne',\n",
              " 'marisela',\n",
              " 'natasha',\n",
              " 'janessa',\n",
              " 'jeannine',\n",
              " 'tabitha',\n",
              " 'karissa',\n",
              " 'englert',\n",
              " 'weldon',\n",
              " 'jess',\n",
              " 'elfrieda',\n",
              " 'han',\n",
              " 'schmid',\n",
              " 'jeter',\n",
              " 'deberry',\n",
              " 'lorenz',\n",
              " 'sexton',\n",
              " 'parrott',\n",
              " 'peabody',\n",
              " \"she's\",\n",
              " 'lisabeth',\n",
              " 'pamula',\n",
              " 'banister',\n",
              " 'yourself',\n",
              " 'cherie',\n",
              " 'lory',\n",
              " 'ellan',\n",
              " 'turk',\n",
              " 'vatu',\n",
              " 'shapiro',\n",
              " 'fishman',\n",
              " 'you',\n",
              " 'petry',\n",
              " 'kareem',\n",
              " 'adell',\n",
              " 'caroyln',\n",
              " 'juli',\n",
              " 'jocelyn',\n",
              " 'georgine',\n",
              " 'chaffin',\n",
              " 'hisako',\n",
              " 'estes',\n",
              " 'mapes',\n",
              " 'tanja',\n",
              " 'dorsey',\n",
              " 'garrick',\n",
              " 'vesta',\n",
              " 'alley',\n",
              " 'luck',\n",
              " 'rolland',\n",
              " 'epperson',\n",
              " 'sykes',\n",
              " 'nikita',\n",
              " 'saying',\n",
              " 'stuckey',\n",
              " 'tasia',\n",
              " 'denney',\n",
              " 'napper',\n",
              " 'yancey',\n",
              " 'abell',\n",
              " 'bloom',\n",
              " 'royal',\n",
              " 'jacquelyne',\n",
              " 'can',\n",
              " 'soucy',\n",
              " 'capone',\n",
              " 'sylvie',\n",
              " 'hansel',\n",
              " 'reynaldo',\n",
              " 'knotts',\n",
              " 'zealand',\n",
              " 'gable',\n",
              " \"they'd\",\n",
              " 'dominque',\n",
              " 'atchison',\n",
              " 'teodoro',\n",
              " 'arletta',\n",
              " 'cristal',\n",
              " 'grayson',\n",
              " 'setsuko',\n",
              " 'rogelio',\n",
              " 'abel',\n",
              " 'coffey',\n",
              " 'yuki',\n",
              " 'stephnie',\n",
              " 'macgregor',\n",
              " 'leftwich',\n",
              " 'mckibben',\n",
              " 'caswell',\n",
              " 'u',\n",
              " 'severson',\n",
              " 'muncy',\n",
              " 'cathrine',\n",
              " 'berneice',\n",
              " 'ferguson',\n",
              " 'corrina',\n",
              " 'guajardo',\n",
              " 'laraine',\n",
              " 'ty',\n",
              " 'daggett',\n",
              " 'ngo',\n",
              " 'parisi',\n",
              " 'downes',\n",
              " 'arbogast',\n",
              " 'mcneal',\n",
              " 'beatris',\n",
              " 'norfleet',\n",
              " 'benjamin',\n",
              " 'jospeh',\n",
              " 't',\n",
              " 'huong',\n",
              " 'ohara',\n",
              " 'cities',\n",
              " 'bertram',\n",
              " 'quinton',\n",
              " 'leatrice',\n",
              " 'magali',\n",
              " 'brunet',\n",
              " 'livia',\n",
              " 'cofer',\n",
              " 'trent',\n",
              " 'doria',\n",
              " 'eliseo',\n",
              " 'garner',\n",
              " 'buchholz',\n",
              " 'kanesha',\n",
              " 'sheryll',\n",
              " 'marilyn',\n",
              " 'martins',\n",
              " 'munn',\n",
              " 'francine',\n",
              " 'tiara',\n",
              " 'weaver',\n",
              " 'sammons',\n",
              " 'zulma',\n",
              " 'bousquet',\n",
              " 'albertina',\n",
              " 'cover',\n",
              " 'darlena',\n",
              " 'hagen',\n",
              " 'simonne',\n",
              " 'mccorkle',\n",
              " 'thora',\n",
              " 'goodson',\n",
              " 'shaner',\n",
              " 'camie',\n",
              " 'sizemore',\n",
              " 'greenleaf',\n",
              " 'hubbard',\n",
              " 'busch',\n",
              " 'boudreau',\n",
              " 'sayers',\n",
              " 'ulloa',\n",
              " 'kavanaugh',\n",
              " 'baier',\n",
              " 'macau',\n",
              " 'freeland',\n",
              " 'adalberto',\n",
              " 'sol',\n",
              " 'hooks',\n",
              " 'alejandro',\n",
              " 'durkee',\n",
              " 'real',\n",
              " 'brummett',\n",
              " 'volk',\n",
              " 'sperling',\n",
              " 'name',\n",
              " 'antonia',\n",
              " 'karoline',\n",
              " 'woodhouse',\n",
              " 'robertson',\n",
              " 'dowling',\n",
              " 'paola',\n",
              " 'benita',\n",
              " 'lemke',\n",
              " 'nesmith',\n",
              " 'torrey',\n",
              " 'ellingson',\n",
              " 'silvana',\n",
              " 'paula',\n",
              " 'pendleton',\n",
              " 'adrienne',\n",
              " 'alaina',\n",
              " 'beckie',\n",
              " 'faviola',\n",
              " 'nieves',\n",
              " 'markle',\n",
              " 'jeanine',\n",
              " 'toner',\n",
              " 'joaquin',\n",
              " 'ontario',\n",
              " 'leonore',\n",
              " 'stiltner',\n",
              " 'wickham',\n",
              " 'ronnie',\n",
              " 'sandra',\n",
              " 'currently',\n",
              " 'modlin',\n",
              " 'hermelinda',\n",
              " 'doyon',\n",
              " 'denyse',\n",
              " 'lucius',\n",
              " 'fowlkes',\n",
              " 'kress',\n",
              " 'pettit',\n",
              " 'lowe',\n",
              " 'go',\n",
              " 'marlyn',\n",
              " 'fudge',\n",
              " 'these',\n",
              " 'hal',\n",
              " 'keyes',\n",
              " 'warrington',\n",
              " 'mitchem',\n",
              " 'needn',\n",
              " 'ignacia',\n",
              " 'parra',\n",
              " 'quarterly',\n",
              " 'latter',\n",
              " 'villanueva',\n",
              " 'tammera',\n",
              " 'jordan',\n",
              " 'synthia',\n",
              " 'valley',\n",
              " 'pitt',\n",
              " 'hare',\n",
              " 'angie',\n",
              " 'oretha',\n",
              " 'gooden',\n",
              " 'irvin',\n",
              " 'christoper',\n",
              " 'lavina',\n",
              " 'gayla',\n",
              " 'wharton',\n",
              " 'murchison',\n",
              " 'palma',\n",
              " 'nana',\n",
              " 'stocker',\n",
              " 'reiter',\n",
              " 'hartman',\n",
              " 'graf',\n",
              " 'quiles',\n",
              " 'kamilah',\n",
              " 'varner',\n",
              " 'dorthea',\n",
              " 'eidson',\n",
              " 'welch',\n",
              " 'free',\n",
              " 'peeler',\n",
              " 'adrianne',\n",
              " 'jacquline',\n",
              " 'mahon',\n",
              " 'badger',\n",
              " 'van',\n",
              " 'pelton',\n",
              " 'rawlings',\n",
              " 'healy',\n",
              " 'letourneau',\n",
              " 'venice',\n",
              " 'jolynn',\n",
              " 'wendolyn',\n",
              " 'lapierre',\n",
              " 'florence',\n",
              " 'brink',\n",
              " 'edgerton',\n",
              " 'angola',\n",
              " 'furtado',\n",
              " 'tries',\n",
              " 'grider',\n",
              " 'viva',\n",
              " 'martel',\n",
              " 'kittrell',\n",
              " 'dan',\n",
              " 'vogel',\n",
              " 'ruch',\n",
              " 'sandberg',\n",
              " 'bulter',\n",
              " 'suzette',\n",
              " 'kathlene',\n",
              " 'manat',\n",
              " 'mullen',\n",
              " 'eck',\n",
              " 'rothstein',\n",
              " 'lucina',\n",
              " 'lavender',\n",
              " 'ellison',\n",
              " 'krieger',\n",
              " 'carruth',\n",
              " 'powers',\n",
              " 'sandler',\n",
              " 'domonique',\n",
              " 'sherman',\n",
              " 'walls',\n",
              " 'goes',\n",
              " 'thao',\n",
              " 'vo',\n",
              " 'woodrow',\n",
              " 'dillingham',\n",
              " 'ivana',\n",
              " 'sinclair',\n",
              " 'antione',\n",
              " 'reina',\n",
              " 'necole',\n",
              " 'tameika',\n",
              " 'olney',\n",
              " 'costa',\n",
              " 'samaniego',\n",
              " 'jesse',\n",
              " 'sturgeon',\n",
              " 'abby',\n",
              " 'christin',\n",
              " 'burmeister',\n",
              " 'sisco',\n",
              " 'pitcher',\n",
              " 'hildebrand',\n",
              " 'heckman',\n",
              " 'hillary',\n",
              " 'maurer',\n",
              " 'bibbs',\n",
              " 'sedillo',\n",
              " 'honduras',\n",
              " 'rod',\n",
              " 'kerrigan',\n",
              " 'heald',\n",
              " 'danelle',\n",
              " 'lillard',\n",
              " 'n',\n",
              " 'such',\n",
              " 'cicely',\n",
              " 'deonna',\n",
              " 'lev',\n",
              " 'copley',\n",
              " 'alena',\n",
              " 'bisson',\n",
              " 'houck',\n",
              " 'dobbs',\n",
              " 'maegan',\n",
              " 'miss',\n",
              " 'starnes',\n",
              " 'deana',\n",
              " 'tovar',\n",
              " 'baumgardner',\n",
              " 'fouts',\n",
              " 'jammie',\n",
              " 'whitson',\n",
              " 'ute',\n",
              " 'knott',\n",
              " 'milton',\n",
              " 'rico',\n",
              " 'groff',\n",
              " 'blaylock',\n",
              " 'huddleston',\n",
              " 'kristy',\n",
              " 'bourgeois',\n",
              " 'torri',\n",
              " 'has',\n",
              " 'jan',\n",
              " '/.post',\n",
              " 'rogers',\n",
              " 'kean',\n",
              " 'matherne',\n",
              " 'moyer',\n",
              " 'renate',\n",
              " 'grimsley',\n",
              " 'some',\n",
              " 'delmar',\n",
              " 'larita',\n",
              " 'sprague',\n",
              " 'kristen',\n",
              " 'wanita',\n",
              " 'mao',\n",
              " 'moreau',\n",
              " 'bierman',\n",
              " 'bey',\n",
              " 'shearer',\n",
              " 'dimple',\n",
              " 'hiller',\n",
              " 'bidwell',\n",
              " 'merrick',\n",
              " 'erb',\n",
              " 'mccartney',\n",
              " 'register',\n",
              " 'kerrie',\n",
              " 'mundy',\n",
              " 'marjorie',\n",
              " 'henshaw',\n",
              " 'renaud',\n",
              " 'redding',\n",
              " 'delp',\n",
              " 'abdul',\n",
              " 'wales',\n",
              " 'needham',\n",
              " 'thomasine',\n",
              " 'natisha',\n",
              " 'rita',\n",
              " 'ayana',\n",
              " 'noll',\n",
              " 'street',\n",
              " 'carylon',\n",
              " 'aquino',\n",
              " 'corrinne',\n",
              " 'qtr',\n",
              " 'carlson',\n",
              " 'berkley',\n",
              " 'beverly',\n",
              " 'ohare',\n",
              " 'olga',\n",
              " 'dressler',\n",
              " 'cecilia',\n",
              " 'merrie',\n",
              " 'wallis',\n",
              " 'wadsworth',\n",
              " 'pak',\n",
              " 'pugliese',\n",
              " 'dion',\n",
              " 'leah',\n",
              " 'heide',\n",
              " 'marni',\n",
              " 'ruthie',\n",
              " 'verlie',\n",
              " 'remona',\n",
              " 'sarai',\n",
              " 'norine',\n",
              " 'krona',\n",
              " 'segovia',\n",
              " 'hochstetler',\n",
              " 'herr',\n",
              " 'charity',\n",
              " 'tora',\n",
              " 'mcmahan',\n",
              " 'fitts',\n",
              " 'pia',\n",
              " 'jennette',\n",
              " 'kim',\n",
              " 'sturdivant',\n",
              " 'barnwell',\n",
              " 'bridgette',\n",
              " 'gerda',\n",
              " 'jerald',\n",
              " 'bauer',\n",
              " 'al',\n",
              " 'mccaffrey',\n",
              " 'cabrera',\n",
              " 'bergeron',\n",
              " 'loni',\n",
              " 'oma',\n",
              " 'marylee',\n",
              " 'frost',\n",
              " 'arica',\n",
              " \"it'd\",\n",
              " 'stadler',\n",
              " 'sharon',\n",
              " 'cÃ³rdoba',\n",
              " 'clint',\n",
              " 'bearden',\n",
              " 'bowser',\n",
              " 'tawny',\n",
              " 'wiles',\n",
              " 'saul',\n",
              " 'tanna',\n",
              " \"i'll\",\n",
              " 'poulos',\n",
              " 'carbaugh',\n",
              " 'spriggs',\n",
              " 'felipa',\n",
              " 'deirdre',\n",
              " 'ione',\n",
              " 'maia',\n",
              " 'vanhorn',\n",
              " 'manchester',\n",
              " 'homer',\n",
              " 'mcmanus',\n",
              " 'amato',\n",
              " 'hatfield',\n",
              " 'clary',\n",
              " 'karan',\n",
              " \"wouldn't\",\n",
              " 'binns',\n",
              " 'bauman',\n",
              " 'panama',\n",
              " 'daisy',\n",
              " 'manson',\n",
              " 'struck',\n",
              " 'boozer',\n",
              " 'ellsworth',\n",
              " 'kpmg',\n",
              " 'j',\n",
              " 'ridley',\n",
              " 'bunn',\n",
              " 'graff',\n",
              " 'particia',\n",
              " 'abbott',\n",
              " 'altamirano',\n",
              " 'bull',\n",
              " 'roberson',\n",
              " 'bernier',\n",
              " 'lue',\n",
              " 'gwyneth',\n",
              " 'australia',\n",
              " 'lucero',\n",
              " 'ness',\n",
              " 'indeed',\n",
              " 'hundred',\n",
              " 'kari',\n",
              " 'marianne',\n",
              " 'bobbi',\n",
              " 'chacon',\n",
              " 'kersey',\n",
              " 'barbie',\n",
              " 'arrington',\n",
              " 'kenner',\n",
              " 'jody',\n",
              " 'rosas',\n",
              " 'frisbie',\n",
              " 'lister',\n",
              " 'paris',\n",
              " 'jo',\n",
              " 'venessa',\n",
              " 'wilcox',\n",
              " 'hildegarde',\n",
              " 'beach',\n",
              " 'kathi',\n",
              " 'at',\n",
              " 'lacey',\n",
              " 'scarberry',\n",
              " 'durand',\n",
              " 'moon',\n",
              " 'nikole',\n",
              " 'lewin',\n",
              " 'gillispie',\n",
              " 'margarett',\n",
              " 'hasty',\n",
              " 'india',\n",
              " 'markita',\n",
              " 'trudeau',\n",
              " 'santo',\n",
              " 'definitely',\n",
              " 'brooks',\n",
              " 'rust',\n",
              " 'paddock',\n",
              " 'delma',\n",
              " 'rosanna',\n",
              " 'joelle',\n",
              " 'neoma',\n",
              " 'dotson',\n",
              " 'pridgen',\n",
              " 'jaquelyn',\n",
              " 'gause',\n",
              " 'mercy',\n",
              " 'arkansas',\n",
              " 'boucher',\n",
              " 'carreon',\n",
              " 'messick',\n",
              " 'brynn',\n",
              " 'mozelle',\n",
              " 'darcel',\n",
              " 'chante',\n",
              " 'kane',\n",
              " 'schuster',\n",
              " 'elwell',\n",
              " 'ellington',\n",
              " 'mcclean',\n",
              " 'conte',\n",
              " 'riddell',\n",
              " 'caldera',\n",
              " 'cassady',\n",
              " 'leif',\n",
              " 'eleanora',\n",
              " 'mitzie',\n",
              " 'herrin',\n",
              " 'nannette',\n",
              " 'tarver',\n",
              " 'blanca',\n",
              " 'leandra',\n",
              " 'michaela',\n",
              " 'samual',\n",
              " 'daniele',\n",
              " 'rainey',\n",
              " 'barabara',\n",
              " 'bain',\n",
              " 'kylie',\n",
              " 'sacco',\n",
              " 'ison',\n",
              " 'kurt',\n",
              " 'patton',\n",
              " 'mcnair',\n",
              " 'genesis',\n",
              " 'pardo',\n",
              " 'robins',\n",
              " 'tallent',\n",
              " 'farrah',\n",
              " 'vania',\n",
              " 'dupre',\n",
              " 'arguello',\n",
              " 'gary',\n",
              " 'sikes',\n",
              " 'hale',\n",
              " 'margot',\n",
              " 'sarina',\n",
              " 'tabb',\n",
              " 'glisson',\n",
              " 'virgina',\n",
              " 'lannie',\n",
              " 'danielson',\n",
              " '(former',\n",
              " 'vena',\n",
              " 'per',\n",
              " 'godwin',\n",
              " 'turney',\n",
              " 'evelia',\n",
              " 'grisham',\n",
              " 'goode',\n",
              " 'wirth',\n",
              " 'others',\n",
              " 'hardaway',\n",
              " 'reavis',\n",
              " 'bouton',\n",
              " 'josephina',\n",
              " 'stephane',\n",
              " 'padron',\n",
              " 'bounds',\n",
              " 'charlette',\n",
              " 'burks',\n",
              " 'herbert',\n",
              " 'preston',\n",
              " 'candi',\n",
              " 'estonia',\n",
              " 'rebbecca',\n",
              " 'griffis',\n",
              " 'mose',\n",
              " 'wingo',\n",
              " 'isreal',\n",
              " 'illinois',\n",
              " 'buena',\n",
              " 'hofmann',\n",
              " 'trainor',\n",
              " 'reanna',\n",
              " 'shedd',\n",
              " 'pearle',\n",
              " 'laverne',\n",
              " 'nikia',\n",
              " 'kiyoko',\n",
              " 'horning',\n",
              " 'gilbert',\n",
              " 'batista',\n",
              " 'keneth',\n",
              " 'shenna',\n",
              " 'southerland',\n",
              " 'turpin',\n",
              " 'oralia',\n",
              " 'mark',\n",
              " 'inocencia',\n",
              " 'iowa',\n",
              " 'lavonda',\n",
              " 'diamond',\n",
              " 'jacinda',\n",
              " 'charley',\n",
              " 'begley',\n",
              " 'fancher',\n",
              " 'tinsley',\n",
              " 'hackworth',\n",
              " 'amada',\n",
              " 'edmund',\n",
              " 'byrd',\n",
              " 'lael',\n",
              " 'fulk',\n",
              " 'riddle',\n",
              " 'plus',\n",
              " 'moats',\n",
              " 'guertin',\n",
              " 'trinity',\n",
              " 'aguilar',\n",
              " 'zachariah',\n",
              " 'drema',\n",
              " 'dansby',\n",
              " 'matthew',\n",
              " 'delgadillo',\n",
              " 'daniels',\n",
              " 'mariann',\n",
              " 'raguel',\n",
              " 'alise',\n",
              " 'nail',\n",
              " 'pender',\n",
              " 'pastor',\n",
              " 'lila',\n",
              " 'carmella',\n",
              " 'shupe',\n",
              " 'cassell',\n",
              " 'haddad',\n",
              " 'keira',\n",
              " 'sam',\n",
              " 'aleta',\n",
              " 'afton',\n",
              " 'torrie',\n",
              " 'when',\n",
              " 'dombrowski',\n",
              " 'mock',\n",
              " 'lorretta',\n",
              " 'pilcher',\n",
              " 'worth',\n",
              " 'paulene',\n",
              " 'bales',\n",
              " 'maritza',\n",
              " 'lindstrom',\n",
              " 'denison',\n",
              " 'dannie',\n",
              " 'indicated',\n",
              " 'grigsby',\n",
              " 'keiser',\n",
              " 'lashon',\n",
              " 'christensen',\n",
              " 'matson',\n",
              " 'crews',\n",
              " 'elana',\n",
              " 'bowen',\n",
              " 'barnhill',\n",
              " 'over',\n",
              " 'beery',\n",
              " 'stancil',\n",
              " 'oubre',\n",
              " 'cropper',\n",
              " 'janetta',\n",
              " 'amal',\n",
              " 'trinidad',\n",
              " 'tran',\n",
              " 'colby',\n",
              " 'nia',\n",
              " 'combs',\n",
              " 'kirkland',\n",
              " 'barnett',\n",
              " 'karri',\n",
              " 'leanna',\n",
              " 'meister',\n",
              " 'sample',\n",
              " 'burris',\n",
              " 'chapman',\n",
              " 'binkley',\n",
              " 'dimaggio',\n",
              " 'maddie',\n",
              " 'cottingham',\n",
              " 'christia',\n",
              " 'mozella',\n",
              " 'linton',\n",
              " 'hamblin',\n",
              " 'pollack',\n",
              " 'rush',\n",
              " 'divina',\n",
              " 'chris',\n",
              " 'maynard',\n",
              " 'olszewski',\n",
              " 'shantel',\n",
              " 'amira',\n",
              " 'sayles',\n",
              " 'crook',\n",
              " 'gabel',\n",
              " 'maybell',\n",
              " 'kortney',\n",
              " 'eliza',\n",
              " 'eugenio',\n",
              " 'twenty',\n",
              " 'eduardo',\n",
              " 'edwin',\n",
              " 'papua',\n",
              " 'cody',\n",
              " 'tiffanie',\n",
              " 'desiree',\n",
              " 'myrtie',\n",
              " 'samples',\n",
              " 'lux',\n",
              " 'sargent',\n",
              " \"it's\",\n",
              " 'carlisle',\n",
              " 'miller',\n",
              " 'littrell',\n",
              " 'coyle',\n",
              " 'showers',\n",
              " 'touche',\n",
              " 'seem',\n",
              " 'gina',\n",
              " 'dwayne',\n",
              " 'demetrius',\n",
              " 'mckean',\n",
              " 'tawanna',\n",
              " 'bundy',\n",
              " 'estefana',\n",
              " 'lorena',\n",
              " 'thibodeaux',\n",
              " 'halter',\n",
              " 'jorgensen',\n",
              " 'lavette',\n",
              " 'nesbitt',\n",
              " 'eubank',\n",
              " 'hallie',\n",
              " 'mittie',\n",
              " 'rosendo',\n",
              " 'skaggs',\n",
              " 'margery',\n",
              " 'indicates',\n",
              " 'fife',\n",
              " 'bostwick',\n",
              " 'mulcahy',\n",
              " 'post',\n",
              " 'chantell',\n",
              " 'rickard',\n",
              " 'baht',\n",
              " 'bayne',\n",
              " 'samantha',\n",
              " 'dover',\n",
              " 'amerson',\n",
              " 'bussell',\n",
              " 'cain',\n",
              " 'daryl',\n",
              " 'malissa',\n",
              " 'bode',\n",
              " 'tammy',\n",
              " 'valrie',\n",
              " 'tarah',\n",
              " 'fink',\n",
              " 'smock',\n",
              " 'trisha',\n",
              " 'didn',\n",
              " 'shaffer',\n",
              " 'season',\n",
              " 'noelle',\n",
              " 'miner',\n",
              " 'philip',\n",
              " 'dorian',\n",
              " 'bynum',\n",
              " 'seymore',\n",
              " 'bell',\n",
              " 'tisdale',\n",
              " 'bolen',\n",
              " 'rosa',\n",
              " 'guerin',\n",
              " 'millar',\n",
              " 'tangela',\n",
              " 'bowes',\n",
              " 'machado',\n",
              " 'lehmann',\n",
              " 'vernia',\n",
              " 'mabry',\n",
              " 'amin',\n",
              " 'randa',\n",
              " 'maricruz',\n",
              " 'julian',\n",
              " 'giovanna',\n",
              " 'chana',\n",
              " 'mcnamee',\n",
              " 'dalila',\n",
              " 'lilliana',\n",
              " 'kratz',\n",
              " 'devora',\n",
              " 'kacey',\n",
              " 'reid',\n",
              " 'schaller',\n",
              " 'suber',\n",
              " 'finch',\n",
              " 'federico',\n",
              " 'latoria',\n",
              " 'brant',\n",
              " 'whatley',\n",
              " 'eggert',\n",
              " 'foley',\n",
              " 'justice',\n",
              " 'pena',\n",
              " 'feller',\n",
              " 'goodell',\n",
              " 'sales',\n",
              " 'childers',\n",
              " 'fuhrman',\n",
              " 'perla',\n",
              " 'duffy',\n",
              " 'rhyne',\n",
              " 'delapaz',\n",
              " 'trevor',\n",
              " 'varney',\n",
              " 'look',\n",
              " 'cuomo',\n",
              " 'lim',\n",
              " 'mildred',\n",
              " 'lilian',\n",
              " 'moran',\n",
              " 'gilley',\n",
              " 'merideth',\n",
              " 'miranda',\n",
              " 'kelsey',\n",
              " 'springer',\n",
              " 'theola',\n",
              " 'cinderella',\n",
              " 'shaver',\n",
              " 'yoon',\n",
              " 'gena',\n",
              " 'silvey',\n",
              " 'khan',\n",
              " 'akiko',\n",
              " 'tibbetts',\n",
              " 'liberty',\n",
              " 'kocher',\n",
              " 'lelah',\n",
              " 'keven',\n",
              " 'hales',\n",
              " 'hadley',\n",
              " 'ji',\n",
              " 'mann',\n",
              " 'kevin',\n",
              " 'knoll',\n",
              " 'benitez',\n",
              " 'kimball',\n",
              " 'wieland',\n",
              " 'hyman',\n",
              " 'mccutcheon',\n",
              " 'rafferty',\n",
              " 'sherrill',\n",
              " 'mi',\n",
              " 'especially',\n",
              " 'robinett',\n",
              " 'lucas',\n",
              " 'someone',\n",
              " 'song',\n",
              " 'macpherson',\n",
              " 'pamala',\n",
              " 'butts',\n",
              " 'withrow',\n",
              " 'adelaida',\n",
              " 'shira',\n",
              " 'zoraida',\n",
              " 'terina',\n",
              " 'colella',\n",
              " 'washington',\n",
              " 'hartfield',\n",
              " 'eveline',\n",
              " 'kathryne',\n",
              " 'ammons',\n",
              " 'corcoran',\n",
              " 'alvaro',\n",
              " 'shea',\n",
              " 'sabine',\n",
              " 'phillips',\n",
              " 'garry',\n",
              " 'away',\n",
              " 'doll',\n",
              " 'cristina',\n",
              " 'jame',\n",
              " 'gladys',\n",
              " 'bernarda',\n",
              " 'alfreda',\n",
              " 'cleora',\n",
              " 'koons',\n",
              " 'eliz',\n",
              " 'cohen',\n",
              " 'penny',\n",
              " 'tennessee',\n",
              " 'richey',\n",
              " 'wally',\n",
              " 'caraballo',\n",
              " 'haggerty',\n",
              " 'latanya',\n",
              " 'zora',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT4yEEYxrg1J"
      },
      "source": [
        "#Negative words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try reading the file with UTF-8 encoding\n",
        "try:\n",
        "    negative_words = open(\"/content/Stopwords/{filename}\".format(filename=\"negative-words.txt\"), 'r', encoding='utf-8').read().strip().lower()\n",
        "except UnicodeDecodeError:\n",
        "    # If UTF-8 decoding fails, try with Latin-1 encoding\n",
        "    negative_words = open(\"/content/Stopwords/{filename}\".format(filename=\"negative-words.txt\"), 'r', encoding='latin-1').read().strip().lower()\n",
        "\n",
        "negative_words = set(negative_words.split())\n"
      ],
      "metadata": {
        "id": "TmGRUsUa9ZZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596XtXzXriaZ"
      },
      "source": [
        "#Positive words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2dsQw0MrTnb"
      },
      "outputs": [],
      "source": [
        "positive_words=open(\"/content/Stopwords/{filename}\".format(filename=\"positive-words.txt\"),'r').read().strip().lower()\n",
        "positive_words=set(positive_words.split())\n",
        "# print(positive_words)\n",
        "# print(len(positive_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeeYjPVjSaWY"
      },
      "outputs": [],
      "source": [
        "# declearing  arrays to store outputs\n",
        "POSITIVE_SCORE=[]\n",
        "NEGATIVE_SCORE=[]\n",
        "POLARITY_SCORE=[]\n",
        "SUBJECTIVITY_SCORE=[]\n",
        "AVG_SENTENCE_LENGTH=[]\n",
        "PercentageOfComplexWords=[]\n",
        "FogIndex=[]\n",
        "AVG_NUMBER_OF_WORDS_PER_SENTENCE=[]\n",
        "COMPLEX_WORD_COUNT=[]\n",
        "WORD_COUNT=[]\n",
        "PERSONAL_PRONOUNS=[]\n",
        "SYLLABLE_PER_WORD=[]\n",
        "AVG_WORD_LENGTH=[]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pandas youtube-transcript-api nltk openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02mgFi4a-gxH",
        "outputId": "81063adf-b40e-4bc3-e0d7-94f6c2770f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.2.2)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/Input.xlsx')\n",
        "\n",
        "# Assuming the YouTube links are in a column named 'YouTube Link'\n",
        "URL = df['URL'].tolist()"
      ],
      "metadata": {
        "id": "CxufQbJc-bxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_video_ids(youtube_links):\n",
        "    video_ids = []\n",
        "    for link in youtube_links:\n",
        "        try:\n",
        "            video_id = link.split('?v=')[1]\n",
        "            video_ids.append(video_id)\n",
        "        except IndexError:\n",
        "            print(f\"Invalid YouTube link: {link}\")\n",
        "    return video_ids\n",
        "\n",
        "# Extract video IDs\n",
        "video_ids = extract_video_ids(URL)\n",
        "\n",
        "# Function to extract transcripts using video IDs\n",
        "def extract_transcripts(video_ids):\n",
        "    transcripts = []\n",
        "    count = 0\n",
        "    for video_id in video_ids:\n",
        "        try:\n",
        "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'en-IN', 'ja', 'hi', 'de'])\n",
        "            transcript = ' '.join([t['text'] for t in transcript_list])\n",
        "            transcripts.append(transcript)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video ID {video_id}: {e}\")\n",
        "            count+=1\n",
        "            transcript = ' '.join(\"\")\n",
        "            transcripts.append(transcript)\n",
        "    return transcripts, count\n",
        "\n",
        "# Extract transcripts\n",
        "extracted_transcripts, count = extract_transcripts(video_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy3f_Yb5AT3r",
        "outputId": "d5914b39-7dee-4705-9b5c-e95fbe4e69e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing video ID wXbRW4-kXqM: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=wXbRW4-kXqM! This is most likely caused by:\n",
            "\n",
            "Subtitles are disabled for this video\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Error processing video ID 9tOtIDjN6_k: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=9tOtIDjN6_k! This is most likely caused by:\n",
            "\n",
            "Subtitles are disabled for this video\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Error processing video ID bViveOxeHQ0: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=bViveOxeHQ0! This is most likely caused by:\n",
            "\n",
            "Subtitles are disabled for this video\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Error processing video ID UtqhLPpJDNg: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=UtqhLPpJDNg! This is most likely caused by:\n",
            "\n",
            "Subtitles are disabled for this video\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Error processing video ID osuhB3_dh0s: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=osuhB3_dh0s! This is most likely caused by:\n",
            "\n",
            "Subtitles are disabled for this video\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ8TPTCAI4PO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b74c0d2-286e-4718-b0ba-48cacabe5296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iterating through each url in list\n",
            "*********** NaN found ************\n",
            "completed URL == 1\n",
            "completed URL == 2\n",
            "completed URL == 3\n",
            "completed URL == 4\n",
            "completed URL == 5\n",
            "*********** NaN found ************\n",
            "completed URL == 6\n",
            "completed URL == 7\n",
            "completed URL == 8\n",
            "completed URL == 9\n",
            "completed URL == 10\n",
            "completed URL == 11\n",
            "*********** NaN found ************\n",
            "completed URL == 12\n",
            "completed URL == 13\n",
            "completed URL == 14\n",
            "completed URL == 15\n",
            "completed URL == 16\n",
            "completed URL == 17\n",
            "completed URL == 18\n",
            "completed URL == 19\n",
            "completed URL == 20\n",
            "completed URL == 21\n",
            "completed URL == 22\n",
            "completed URL == 23\n",
            "completed URL == 24\n",
            "completed URL == 25\n",
            "completed URL == 26\n",
            "completed URL == 27\n",
            "completed URL == 28\n",
            "completed URL == 29\n",
            "completed URL == 30\n",
            "completed URL == 31\n",
            "completed URL == 32\n",
            "completed URL == 33\n",
            "*********** NaN found ************\n",
            "completed URL == 34\n",
            "completed URL == 35\n",
            "completed URL == 36\n",
            "completed URL == 37\n",
            "completed URL == 38\n",
            "completed URL == 39\n",
            "completed URL == 40\n",
            "completed URL == 41\n",
            "completed URL == 42\n",
            "completed URL == 43\n",
            "completed URL == 44\n",
            "completed URL == 45\n",
            "completed URL == 46\n",
            "completed URL == 47\n",
            "completed URL == 48\n",
            "completed URL == 49\n",
            "completed URL == 50\n",
            "completed URL == 51\n",
            "completed URL == 52\n",
            "completed URL == 53\n",
            "*********** NaN found ************\n",
            "completed URL == 54\n",
            "completed URL == 55\n",
            "completed URL == 56\n",
            "completed URL == 57\n",
            "completed URL == 58\n",
            "completed URL == 59\n",
            "completed URL == 60\n",
            "completed URL == 61\n"
          ]
        }
      ],
      "source": [
        "url_no=0\n",
        "print(\"iterating through each url in list\")\n",
        "#iterating through each url in list\n",
        "for cleaned_text in extracted_transcripts:\n",
        "  url_no += 1\n",
        "  # for cleaned_text in text_content:\n",
        "\n",
        "  if str(cleaned_text) != '': #cleaning request (text_content) if it has any nan value\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    # 1. Create an NLP document with Spacy:\n",
        "    doc = nlp(cleaned_text)\n",
        "    input_string=str(doc).lower().strip().split()\n",
        "    # print(\"doc=--------->>>>\\n\",doc)\n",
        "\n",
        "    input_string= str(doc)\n",
        "    input_string=input_string.lower().split()\n",
        "    tokens_without_sw = [word for word in input_string if not word in en_stopwords]\n",
        "\n",
        "\n",
        "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
        "    pronouns = pronounRegex.findall(str(doc))\n",
        "\n",
        "    all_words=0\n",
        "    negative_word_count = 0\n",
        "    positive_word_count = 0\n",
        "    complex_word,syllable_count,syllable_word_count =0,0,0\n",
        "    number_of_sentences=len(list(doc.sents))\n",
        "\n",
        "    for i in tokens_without_sw:\n",
        "      all_words+=1 # word counter\n",
        "\n",
        "\n",
        "      if i in negative_words: # negative word pass\n",
        "        negative_word_count+=1 # negative word counter\n",
        "\n",
        "      if i in positive_words: # positive word pass\n",
        "        positive_word_count+=1 # positive word counter\n",
        "\n",
        "      complex_word,syllable_count,syllable_word_count=syllable_and_count(i,syllable_count,syllable_word_count,complex_word) #calling syllable_and_count function\n",
        "    if all_words==0:\n",
        "      print(\"*********** All Words are zero ************\")\n",
        "\n",
        "      #setting all valuse to NAN if link responce is NAN\n",
        "      POSITIVE_SCORE.append(\"0\")\n",
        "      NEGATIVE_SCORE.append(\"0\")\n",
        "      POLARITY_SCORE.append(\"0\")\n",
        "      SUBJECTIVITY_SCORE.append(\"0\")\n",
        "      AVG_SENTENCE_LENGTH.append(\"0\")\n",
        "      PercentageOfComplexWords.append(\"0\")\n",
        "      FogIndex.append(\"0\")\n",
        "      AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(\"0\")\n",
        "      COMPLEX_WORD_COUNT.append(\"0\")\n",
        "      WORD_COUNT.append(\"0\")\n",
        "      SYLLABLE_PER_WORD.append(\"0\")\n",
        "      PERSONAL_PRONOUNS.append(\"0\")\n",
        "      AVG_WORD_LENGTH.append(\"0\")\n",
        "      continue\n",
        "\n",
        "\n",
        "    polarity_score = (positive_word_count - negative_word_count) / ((positive_word_count + negative_word_count) + 0.000001) # calculating plock\n",
        "    subjectivity_score = (positive_word_count - negative_word_count) / (all_words + 0.000001) # calculating subjectivity_score\n",
        "\n",
        "    Average_Sentence_Length = all_words / number_of_sentences\n",
        "    Percentage_of_Complex_words = complex_word / all_words\n",
        "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
        "\n",
        "\n",
        "    #appending calculation to the specific list Respectively\n",
        "    POSITIVE_SCORE.append(positive_word_count)\n",
        "    NEGATIVE_SCORE.append(negative_word_count)\n",
        "    POLARITY_SCORE.append(polarity_score)\n",
        "    SUBJECTIVITY_SCORE.append(subjectivity_score)\n",
        "    AVG_SENTENCE_LENGTH.append(Average_Sentence_Length)\n",
        "    PercentageOfComplexWords.append(Percentage_of_Complex_words)\n",
        "    FogIndex.append(Fog_Index)\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(len(doc)/Average_Sentence_Length)\n",
        "    COMPLEX_WORD_COUNT.append(complex_word)\n",
        "    WORD_COUNT.append(all_words)\n",
        "    SYLLABLE_PER_WORD.append(syllable_count/syllable_word_count)\n",
        "    PERSONAL_PRONOUNS.append(len(pronouns))\n",
        "    AVG_WORD_LENGTH.append(len(doc)/all_words)\n",
        "\n",
        "  else:\n",
        "    print(\"*********** NaN found ************\")\n",
        "\n",
        "    #setting all valuse to NAN if link responce is NAN\n",
        "    POSITIVE_SCORE.append(\"nan\")\n",
        "    NEGATIVE_SCORE.append(\"nan\")\n",
        "    POLARITY_SCORE.append(\"nan\")\n",
        "    SUBJECTIVITY_SCORE.append(\"nan\")\n",
        "    AVG_SENTENCE_LENGTH.append(\"nan\")\n",
        "    PercentageOfComplexWords.append(\"nan\")\n",
        "    FogIndex.append(\"nan\")\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(\"nan\")\n",
        "    COMPLEX_WORD_COUNT.append(\"nan\")\n",
        "    WORD_COUNT.append(\"nan\")\n",
        "    SYLLABLE_PER_WORD.append(\"nan\")\n",
        "    PERSONAL_PRONOUNS.append(\"nan\")\n",
        "    AVG_WORD_LENGTH.append(\"nan\")\n",
        "\n",
        "\n",
        "  print(f\"completed URL == {url_no}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(POSITIVE_SCORE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXAmNcWsEUUh",
        "outputId": "57a4fb72-8fa3-4840-dd7c-b3a93a60d36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVcUHZs6W6mK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "ec4c68d4-4532-41b4-e8be-a5eaa4252f73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            URL POSITIVE SCORE NEGATIVE SCORE  \\\n",
              "0   https://www.youtube.com/watch?v=wXbRW4-kXqM            nan            nan   \n",
              "1   https://www.youtube.com/watch?v=iL3CLJVaG9U            nan            nan   \n",
              "2   https://www.youtube.com/watch?v=eug1sbP5Y-g            nan            nan   \n",
              "3   https://www.youtube.com/watch?v=3OBFc0C9TbM              5             19   \n",
              "4   https://www.youtube.com/watch?v=EOyYUnf8MT0              5             28   \n",
              "..                                          ...            ...            ...   \n",
              "56  https://www.youtube.com/watch?v=ytPbO_lgwAs             14             31   \n",
              "57  https://www.youtube.com/watch?v=s2SVe7aDoVQ              1              4   \n",
              "58  https://www.youtube.com/watch?v=WFcLfDSQuRc              0              8   \n",
              "59  https://www.youtube.com/watch?v=J3nJN6jD2BI              1              2   \n",
              "60  https://www.youtube.com/watch?v=HO58sL4T3N0              2             14   \n",
              "\n",
              "   POLARITY SCORE SUBJECTIVITY SCORE AVG SENTENCE LENGTH  \\\n",
              "0             nan                nan                 nan   \n",
              "1             nan                nan                 nan   \n",
              "2             nan                nan                 nan   \n",
              "3       -0.583333          -0.092105                 8.0   \n",
              "4        -0.69697          -0.093496           22.363636   \n",
              "..            ...                ...                 ...   \n",
              "56      -0.377778           -0.05136              41.375   \n",
              "57           -0.6          -0.022556             4.15625   \n",
              "58           -1.0          -0.029963               66.75   \n",
              "59      -0.333333          -0.033333                3.75   \n",
              "60          -0.75           -0.07947            6.565217   \n",
              "\n",
              "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX AVG NUMBER OF WORDS PER SENTENCE  \\\n",
              "0                          nan        nan                              nan   \n",
              "1                          nan        nan                              nan   \n",
              "2                          nan        nan                              nan   \n",
              "3                     0.605263   3.442105                             55.0   \n",
              "4                     0.520325   9.153585                        36.711382   \n",
              "..                         ...        ...                              ...   \n",
              "56                    0.586103  16.784441                        24.725076   \n",
              "57                    0.548872   1.882049                        93.593985   \n",
              "58                    0.464419  26.885768                         4.299625   \n",
              "59                         0.6       1.74                             20.8   \n",
              "60                    0.649007    2.88569                             69.0   \n",
              "\n",
              "   COMPLEX WORD COUNT WORD COUNT SYLLABLE PER WORD PERSONAL PRONOUNS  \\\n",
              "0                 nan        nan               nan               nan   \n",
              "1                 nan        nan               nan               nan   \n",
              "2                 nan        nan               nan               nan   \n",
              "3                  92        152                 7          1.638158   \n",
              "4                 128        246                23           1.48374   \n",
              "..                ...        ...               ...               ...   \n",
              "56                194        331                21           1.55287   \n",
              "57                 73        133                 5          1.526316   \n",
              "58                124        267                 0          1.558052   \n",
              "59                 18         30                 0          1.633333   \n",
              "60                 98        151                12          1.847682   \n",
              "\n",
              "   AVG WORD LENGTH  \n",
              "0              nan  \n",
              "1              nan  \n",
              "2              nan  \n",
              "3         2.894737  \n",
              "4         3.337398  \n",
              "..             ...  \n",
              "56        3.090634  \n",
              "57        2.924812  \n",
              "58        1.074906  \n",
              "59             2.6  \n",
              "60             3.0  \n",
              "\n",
              "[61 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c74bf16-d58e-4469-ab07-a44efc799155\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL</th>\n",
              "      <th>POSITIVE SCORE</th>\n",
              "      <th>NEGATIVE SCORE</th>\n",
              "      <th>POLARITY SCORE</th>\n",
              "      <th>SUBJECTIVITY SCORE</th>\n",
              "      <th>AVG SENTENCE LENGTH</th>\n",
              "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
              "      <th>FOG INDEX</th>\n",
              "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
              "      <th>COMPLEX WORD COUNT</th>\n",
              "      <th>WORD COUNT</th>\n",
              "      <th>SYLLABLE PER WORD</th>\n",
              "      <th>PERSONAL PRONOUNS</th>\n",
              "      <th>AVG WORD LENGTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.youtube.com/watch?v=wXbRW4-kXqM</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.youtube.com/watch?v=iL3CLJVaG9U</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.youtube.com/watch?v=eug1sbP5Y-g</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.youtube.com/watch?v=3OBFc0C9TbM</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.583333</td>\n",
              "      <td>-0.092105</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>3.442105</td>\n",
              "      <td>55.0</td>\n",
              "      <td>92</td>\n",
              "      <td>152</td>\n",
              "      <td>7</td>\n",
              "      <td>1.638158</td>\n",
              "      <td>2.894737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.youtube.com/watch?v=EOyYUnf8MT0</td>\n",
              "      <td>5</td>\n",
              "      <td>28</td>\n",
              "      <td>-0.69697</td>\n",
              "      <td>-0.093496</td>\n",
              "      <td>22.363636</td>\n",
              "      <td>0.520325</td>\n",
              "      <td>9.153585</td>\n",
              "      <td>36.711382</td>\n",
              "      <td>128</td>\n",
              "      <td>246</td>\n",
              "      <td>23</td>\n",
              "      <td>1.48374</td>\n",
              "      <td>3.337398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>https://www.youtube.com/watch?v=ytPbO_lgwAs</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>-0.377778</td>\n",
              "      <td>-0.05136</td>\n",
              "      <td>41.375</td>\n",
              "      <td>0.586103</td>\n",
              "      <td>16.784441</td>\n",
              "      <td>24.725076</td>\n",
              "      <td>194</td>\n",
              "      <td>331</td>\n",
              "      <td>21</td>\n",
              "      <td>1.55287</td>\n",
              "      <td>3.090634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>https://www.youtube.com/watch?v=s2SVe7aDoVQ</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.022556</td>\n",
              "      <td>4.15625</td>\n",
              "      <td>0.548872</td>\n",
              "      <td>1.882049</td>\n",
              "      <td>93.593985</td>\n",
              "      <td>73</td>\n",
              "      <td>133</td>\n",
              "      <td>5</td>\n",
              "      <td>1.526316</td>\n",
              "      <td>2.924812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>https://www.youtube.com/watch?v=WFcLfDSQuRc</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.029963</td>\n",
              "      <td>66.75</td>\n",
              "      <td>0.464419</td>\n",
              "      <td>26.885768</td>\n",
              "      <td>4.299625</td>\n",
              "      <td>124</td>\n",
              "      <td>267</td>\n",
              "      <td>0</td>\n",
              "      <td>1.558052</td>\n",
              "      <td>1.074906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>https://www.youtube.com/watch?v=J3nJN6jD2BI</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.333333</td>\n",
              "      <td>-0.033333</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.74</td>\n",
              "      <td>20.8</td>\n",
              "      <td>18</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1.633333</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>https://www.youtube.com/watch?v=HO58sL4T3N0</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>-0.07947</td>\n",
              "      <td>6.565217</td>\n",
              "      <td>0.649007</td>\n",
              "      <td>2.88569</td>\n",
              "      <td>69.0</td>\n",
              "      <td>98</td>\n",
              "      <td>151</td>\n",
              "      <td>12</td>\n",
              "      <td>1.847682</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>61 rows Ã 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c74bf16-d58e-4469-ab07-a44efc799155')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c74bf16-d58e-4469-ab07-a44efc799155 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c74bf16-d58e-4469-ab07-a44efc799155');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dbb31712-ec35-4ee2-a74c-52643cf5d911\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dbb31712-ec35-4ee2-a74c-52643cf5d911')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dbb31712-ec35-4ee2-a74c-52643cf5d911 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_79c91e27-b55a-4dba-acff-a808b6ae0dbf\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_79c91e27-b55a-4dba-acff-a808b6ae0dbf button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 61,\n  \"fields\": [\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 57,\n        \"samples\": [\n          \"https://www.youtube.com/watch?v=wXbRW4-kXqM\",\n          \"https://www.youtube.com/watch?v=9tOtIDjN6_k\",\n          \"https://www.youtube.com/watch?v=mORRiCkOdq8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POSITIVE SCORE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"nan\",\n          41,\n          56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NEGATIVE SCORE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          4,\n          12,\n          33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POLARITY SCORE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 46,\n        \"samples\": [\n          -0.4749999940625001,\n          -0.7999999200000081,\n          -0.9999990000010001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SUBJECTIVITY SCORE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          -0.06249999609375024,\n          -0.008403361273921333,\n          -0.09493670825989425\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AVG SENTENCE LENGTH\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          3.606060606060606,\n          29.46153846153846,\n          41.375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PERCENTAGE OF COMPLEX WORDS\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.0,\n          0.5488721804511278,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FOG INDEX\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          1.6541889483065955,\n          12.002892147017473,\n          16.784441087613295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AVG NUMBER OF WORDS PER SENTENCE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          95.39495798319328,\n          39.0,\n          24.725075528700906\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"COMPLEX WORD COUNT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 44,\n        \"samples\": [\n          553,\n          45,\n          91\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WORD COUNT\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 44,\n        \"samples\": [\n          119,\n          427,\n          111\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SYLLABLE PER WORD\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          21,\n          4,\n          18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PERSONAL PRONOUNS\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          0.0,\n          1.5263157894736843,\n          1.6333333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AVG WORD LENGTH\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          1.0066445182724253,\n          3.1220657276995305,\n          1.0060422960725075\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# creating cata frame\n",
        "df = pd.DataFrame(columns = ['URL' ,  'POSITIVE SCORE' , 'NEGATIVE SCORE' ,'POLARITY SCORE' ,'SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])\n",
        "\n",
        "# Adding values to dataframe\n",
        "for i in range (0,61):\n",
        "# for i in range (394,490):\n",
        "\n",
        "  list_1 = [\n",
        "            URL[i],\n",
        "            POSITIVE_SCORE[i],\n",
        "            NEGATIVE_SCORE[i],\n",
        "            POLARITY_SCORE[i],\n",
        "            SUBJECTIVITY_SCORE[i],\n",
        "            AVG_SENTENCE_LENGTH[i],\n",
        "            PercentageOfComplexWords[i],\n",
        "            FogIndex[i],\n",
        "            AVG_NUMBER_OF_WORDS_PER_SENTENCE[i],\n",
        "            COMPLEX_WORD_COUNT[i],\n",
        "            WORD_COUNT[i],\n",
        "            PERSONAL_PRONOUNS[i],\n",
        "            SYLLABLE_PER_WORD[i],\n",
        "            AVG_WORD_LENGTH[i]\n",
        "            ]\n",
        "  df.loc[len(df)] = list_1\n",
        "\n",
        "#setting file name\n",
        "file_name = 'OutPut_1.xlsx'\n",
        "\n",
        "# saving the excel\n",
        "df.to_excel(file_name,index=False)\n",
        "# display\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4TqU9p5bWYF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}